{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "torch.set_float32_matmul_precision('medium')  # or 'medium'\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "from lightning import seed_everything\n",
    "from time import time\n",
    "\n",
    "from os import putenv\n",
    "putenv(\"HSA_OVERRIDE_GFX_VERSION\", \"11.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4aed1-1e0c-4895-b96f-9dcfca679dd0",
   "metadata": {},
   "source": [
    "# LSTM from scratch (no training)\n",
    "\n",
    "Just like we have done in previous tutorials, building a neural network, and a Long Short-Term Memory (LSTM) unit is a type of neural network, means we need to create a new class. To make it easy to train the LSTM, this class will inherit from `LightningModule` and we'll create the following methods:\n",
    "- `__init__()` to initialize the Weights and Biases and keep track of a few other house keeping things.\n",
    "- `lstm_unit()` to do the LSTM math. For example, to calculate the percentage of the long-term memory to remember.\n",
    "- `forward()` to make a forward pass through the unrolled LSTM. In other words `forward()` calls `lstm_unit()` for each data point.\n",
    "- `configure_optimizers()` to configure the opimimizer. In the past, we have use `SGD` (Stochastic Gradient Descent), however, in this tutorial we'll change things up and use `Adam`, another popular algorithm for optimizing the Weights and Biases.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss and to keep track of the loss values in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4274850-fac0-4099-87ce-342d0aab2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are implementing an LSTM network by hand...\n",
    "class LSTMbyHand(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        ## The first thing we do is set the seed for the random number generorator.\n",
    "        ## This ensures that when someone creates a model from this class, that model\n",
    "        ## will start off with the exact same random numbers as I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Initialize the tensors for the LSTM\n",
    "        ##\n",
    "        ###################\n",
    "        \n",
    "        ## NOTE: nn.LSTM() uses random values from a uniform distribution to initialize the tensors\n",
    "        ## Here we can do it 2 different ways 1) Normal Distribution and 2) Uniform Distribution\n",
    "        ## We'll start with the Normal Distribtion...\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)        \n",
    "        \n",
    "        ## NOTE: In this case, I'm only using the normal distribution for the Weights.\n",
    "        ## All Biases are initialized to 0.\n",
    "        ##\n",
    "        ## These are the Weights and Biases in the first stage, which determines what percentage\n",
    "        ## of the long-term memory the LSTM unit will remember.\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        ## These are the Weights and Biases in the second stage, which determins the new\n",
    "        ## potential long-term memory and what percentage will be remembered.\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        ## These are the Weights and Biases in the third stage, which determines the\n",
    "        ## new short-term memory and what percentage will be sent to the output.\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        ## We can also initialize all Weights and Biases using a uniform distribution. This is\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        ## lstm_unit does the math for a single LSTM unit.\n",
    "        \n",
    "        ## long term memory is also called \"cell state\"\n",
    "        ## short term memory is also called \"hidden state\"\n",
    "        \n",
    "        # i)\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) + \n",
    "                                              (input_value * self.wlr2) + \n",
    "                                              self.blr1)\n",
    "        \n",
    "        # ii)\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) + \n",
    "                                                   (input_value * self.wpr2) + \n",
    "                                                   self.bpr1)\n",
    "        # iii)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + \n",
    "                                      (input_value * self.wp2) + \n",
    "                                      self.bp1)\n",
    "        \n",
    "        # Long term memory update\n",
    "        updated_long_memory = ((long_memory * long_remember_percent) + \n",
    "                       (potential_remember_percent * potential_memory))\n",
    "        \n",
    "        # iv)\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) + \n",
    "                                       (input_value * self.wo2) + \n",
    "                                       self.bo1)\n",
    "        # v), short term memory update         \n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        return([updated_long_memory, updated_short_memory])\n",
    "        \n",
    "    \n",
    "    def forward(self, input): \n",
    "        ## forward() unrolls the LSTM for the training data by calling lstm_unit() for each day of training data \n",
    "        ## that we have. forward() also keeps track of the long and short-term memories after each day and returns\n",
    "        ## the final short-term memory, which is the 'output' of the LSTM.\n",
    "        \n",
    "        long_memory = 0 # long term memory is also called \"cell state\" and indexed with c0, c1, ..., cN\n",
    "        short_memory = 0 # short term memory is also called \"hidden state\" and indexed with h0, h1, ..., cN\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "        \n",
    "        ## Day 1\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 2\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 3\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 4\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "        \n",
    "        ##### Now return short_memory, which is the 'output' of the LSTM.\n",
    "        return short_memory\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        # return Adam(self.parameters(), lr=0.1) # NOTE: Setting the learning rate to 0.1 trains way faster than\n",
    "                                                 # using the default learning rate, lr=0.001, which requires a lot more \n",
    "                                                 # training. However, if we use the default value, we get \n",
    "                                                 # the exact same Weights and Biases that I used in\n",
    "                                                 # the LSTM Clearly Explained StatQuest video. So we'll use the\n",
    "                                                 # default value.\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        ## NOTE: Our dataset consists of two sequences of values representing Company A and Company B\n",
    "        ## For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,\n",
    "        ## the goal is to predict that the value on Day 5 = 1. We use label_i, the value we want to\n",
    "        ## predict, to keep track of which company we just made a prediction for and \n",
    "        ## log that output value in a company specific file\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74907-ac4c-4dfd-aa53-78eaa1d6cbbb",
   "metadata": {},
   "source": [
    "Once we have created the class that defines an LSTM, we can use it to create a model and print out the randomly initialized Weights and Biases. Then, just for fun, we'll see what those random Weights and Biases predict for **Company A** and **Company B**. If they are good predictions, then we're done! However, the chances of getting good predictions from random values is very small. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3ea653-f746-46db-9029-44242a1b8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "wlr1 tensor(0.3367)\n",
      "wlr2 tensor(0.1288)\n",
      "blr1 tensor(0.)\n",
      "wpr1 tensor(0.2345)\n",
      "wpr2 tensor(0.2303)\n",
      "bpr1 tensor(0.)\n",
      "wp1 tensor(-1.1229)\n",
      "wp2 tensor(-0.1863)\n",
      "bp1 tensor(0.)\n",
      "wo1 tensor(2.2082)\n",
      "wo2 tensor(-0.6380)\n",
      "bo1 tensor(0.)\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(-0.0377)\n",
      "Company B: Observed = 1, Predicted = tensor(-0.0383)\n"
     ]
    }
   ],
   "source": [
    "## Create the model object, print out parameters and see how well\n",
    "## the untrained LSTM can make predictions...\n",
    "model = LSTMbyHand() \n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "## NOTE: To make predictions, we pass in the first 4 days worth of stock values \n",
    "## in an array for each company. In this case, the only difference between the\n",
    "## input values for Company A and B occurs on the first day. Company A has 0 and\n",
    "## Company B has 1.\n",
    "print(\"Company A: Observed = 0, Predicted =\", \n",
    "      model(torch.tensor([0., 0.5, 0.25, 1.])).detach())  # .detach() to not return the gradient\n",
    "print(\"Company B: Observed = 1, Predicted =\", \n",
    "      model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329d6f2-39ff-4f9b-bc99-fbd1a2013815",
   "metadata": {},
   "source": [
    "With the unoptimized paramters, the predicted value for **Company A**, **-0.0377**, isn't terrible, since it is relatively close to the observed value, **0**. However, the predicted value for **Company B**, **-0.0383**, _is_ terrible, because it is relatively far from the observed value, **1**. So, that means we need to train the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1116de-c45d-4d82-a727-73a5806fa18f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0529b9c-c550-4fc6-a424-069e34b29c4b",
   "metadata": {},
   "source": [
    "# LSTM from scratch - with training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e2424-649a-44ce-8a34-c98b8fced609",
   "metadata": {},
   "source": [
    "Since we are using **Lightning** training, training the LSTM we created by hand is pretty easy. All we have to do is create the training data and put it into a `DataLoader`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea2ef9-7fe2-4bfe-8645-4db921a4b20c",
   "metadata": {},
   "source": [
    "...and then create a **Lightning Trainer**, `L.Trainer`, and fit it to the training data. **NOTE:** We are starting with **2000** epochs. This may be enough to successfully optimize all of the parameters, but it might not. We'll find out after we compare the predictions to the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10157397-3100-4d18-be96-cdc294ce89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/atonello/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/atonello/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "HIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m) \u001b[38;5;66;03m# with default learning rate, 0.001 (this tiny learning rate makes learning slow)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(time()\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:208\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:265\u001b[0m, in \u001b[0;36m_FitLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m limits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dl \u001b[38;5;129;01min\u001b[39;00m combined_loader\u001b[38;5;241m.\u001b[39mflattened:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# determine number of batches\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dl) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_len_all_ranks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_zero_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m     num_batches \u001b[38;5;241m=\u001b[39m _parse_num_batches(stage, length, trainer\u001b[38;5;241m.\u001b[39mlimit_train_batches)\n\u001b[1;32m    267\u001b[0m     limits\u001b[38;5;241m.\u001b[39mappend(num_batches)\n",
      "File \u001b[0;32m~/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/utilities/data.py:105\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, strategy, allow_zero_length_dataloader_with_multiple_devices)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    104\u001b[0m total_length \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mreduce(torch\u001b[38;5;241m.\u001b[39mtensor(local_length, device\u001b[38;5;241m=\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mroot_device), reduce_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtotal_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[1;32m    106\u001b[0m     rank_zero_warn(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal length of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataloader)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` across ranks is zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please make sure this was your intention.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m local_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: HIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(accelerator=\"gpu\", max_epochs=2000) # with default learning rate, 0.001 (this tiny learning rate makes learning slow)\n",
    "start = time()\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586093b-38e5-431a-a7f0-ad65f2a4c979",
   "metadata": {},
   "source": [
    "Now that we've trained the model with **2000** epochs, we can see how good the predictions are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef42b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+rocm5.7\n",
      "Is ROCm available: True\n",
      "Device name: AMD Radeon RX 7800 XT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Is ROCm available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb6a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.4342)\n",
      "Company B: Observed = 1, Predicted = tensor(0.6171)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292d775-336f-41b7-838c-b297c49693a8",
   "metadata": {},
   "source": [
    "Unfortunately, these predictions are terrible. :( So it seems like we'll have to do more training. However, it would be awesome if we could be confident that more training will actually improve the predictions. If not, we can spare ourselves a lot of time, and potentially money, and just give up. So, before we dive into more training, let's look at the loss values and predictions that we saved in log files with **TensorBoard**. **TensorBoard** will graph everything that we logged during training, making it super easy to see if things are headed in the right direction or not.\n",
    "\n",
    "To get TensorBoard working\n",
    "- In the Jupyter browser window, go to the **File** menu and select **New**.\n",
    "- In the submenu, select **Terminal**.\n",
    "- In the terminal, navigate to the same directory that contains the **lightning_logs** directory.\n",
    "- Then in the terminal, enter `tensorboard --logdir=lightning_logs/` to start the **TensorBoard** server.\n",
    "- When the **TensorBoard** server starts, it will print out a URL that looks like this `http://localhost:6006/`. Copy the URL and paste it into a new browser window and then you are good to go!!! \n",
    "- BAM!!!\n",
    "\n",
    "Below are the graphs of **loss** (`train_loss`), the predictions for **Company A** (`out_0`), and the predictions for **Company B** (`out_1`). Remember for **Companay A**, we want to predict **0** and for **Company B**, we want to predict **1**.\n",
    "\n",
    "<img src=\"./images/train_loss_2000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_2000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_2000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "**NOTE:** If your graphs look messed up and you see a bunch of different lines, instead of just one red line per graph, then check where this notebook is saved for a directory called `lightning_logs`. Delete `lightning_logs` and the re-run everything in this notebook. One source of problems with the graphs is that every time we train a model, a new batch of log files is created and stored in `lightning_logs` and **TensorBoard**, by default, will plot all of them. You can turn off unwanted log files in **TensorBoard**, and we'll do this later on in this notebook, but for now, the easiest thing to do is to start with a clean slate.\n",
    "\n",
    "Anyway, if we look at the **loss** (`train_loss`), we see that it is going down, which is good, but it still has further to go. When we look at the predictions for **Company A** (`out_0`), we see that they started out pretty good, close to **0**, but then got really bad early on in training, shooting all the way up to **0.5**, but are starting to get smaller. In contrast, when we look at the predictions for **Company B** (`out_1`), we see that they started out really bad, close to **0**, but have been getting better ever since and look like they could continue to get better if we kept training.\n",
    "\n",
    "In summary, the graphs seem to suggest that if we continued training our model, the predictions would improve. So let's add more epochs to the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ed041-4f46-4dc6-bd6f-7d2bf01886a4",
   "metadata": {},
   "source": [
    "<a id=\"add_epochs\"></a>\n",
    "# Optimizing (Training) the Weights and Biases in the LSTM that we made by hand: Part 2 - Adding More Epochs without Starting Over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4989ee2-f2ce-40c1-b8a6-0f3c4d4e06c6",
   "metadata": {},
   "source": [
    "The good news is that because we're using **Lightning**, we can pick up where we left off training without having to start over from scratch. This is because when we train with **Lightning**, it creates _checkpoint_ files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the `Trainer` where the checkpoint files are located. This is awesome and will save us a lot of time since we don't have to retrain the first **2000** epochs. So let's add an additional **1000** epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff55c1-cdc2-4086-b0d4-8d8333058674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: /home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_38/checkpoints/epoch=1999-step=4000.ckpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_38/checkpoints/epoch=1999-step=4000.ckpt\n",
      "/home/atonello/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_38/checkpoints' to '/home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_39/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_38/checkpoints/epoch=1999-step=4000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: 100%|██████████| 2/2 [00:00<00:00, 184.32it/s, v_num=39]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2999: 100%|██████████| 2/2 [00:00<00:00, 135.66it/s, v_num=39]\n"
     ]
    }
   ],
   "source": [
    "## First, find where the most recent checkpoint files are stored\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=3000) # Before, max_epochs=2000, so, by setting it to 3000, we're adding 1000 more.\n",
    "## And then call fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f6af6-7d7e-4c40-9bbf-f467114d838e",
   "metadata": {},
   "source": [
    "Now that we have added **1000** epochs to the training, let's check the predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c805e2b-27ae-4302-af26-5fe0d55f6b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.2708)\n",
      "Company B: Observed = 1, Predicted = tensor(0.7534)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42455342-f774-43a7-a043-536099d39b01",
   "metadata": {},
   "source": [
    "...and they are much better than before. Hooray!!! We can also check the logs with **TensorBoard** to see if it makes sense to add more epochs to the training. Since we already have **TensorBoard** running in a separate browser window, all we have to do is reload that page to update the graphs (below).\n",
    "\n",
    "<img src=\"./images/train_loss_3000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_3000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_3000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "The blue lines in each graph represents the values we logged during the extra **1000** epochs. The **loss** is getting smaller and the predictions for both companies are improving! Hooray!!! However, because it looks like there is even more room for improvement, let's add **2000** more epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb14b21-2aa4-4d37-a58c-d2fb789c06c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: /home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_39/checkpoints/epoch=2999-step=6000.ckpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_39/checkpoints/epoch=2999-step=6000.ckpt\n",
      "/home/atonello/Documents/Gruppo Lavoro LVMH/myvenv/lib64/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_39/checkpoints' to '/home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_40/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /home/atonello/Documents/Gruppo Lavoro LVMH/LSTM/lightning_logs/version_39/checkpoints/epoch=2999-step=6000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|██████████| 2/2 [00:00<00:00, 222.67it/s, v_num=40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|██████████| 2/2 [00:00<00:00, 159.27it/s, v_num=40]\n"
     ]
    }
   ],
   "source": [
    "## First, find where the most recent checkpoint files are stored\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=5000) # Before, max_epochs=3000, so, by setting it to 5000, we're adding 2000 more.\n",
    "## And then call fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231699e4-56eb-4c1b-b48c-e146b8d424fd",
   "metadata": {},
   "source": [
    "Now that we have added **2000** more epochs to the training (for a total of **5000** epochs), let's check the predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c653dc-54ed-4c1a-aca0-419e2311f75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.0022)\n",
      "Company B: Observed = 1, Predicted = tensor(0.9693)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4b82e-6635-4dcd-b889-8374ecb93dad",
   "metadata": {},
   "source": [
    "...and they look good!!! The prediction for **Company A** is super close to **0**, which is exactly what we want, and the prediction for **Company B** is close to **1**, which is also what we want. \n",
    "\n",
    "Now let's look at the graphs in **TensorBoard** by reloading that page.\n",
    "\n",
    "<img src=\"./images/train_loss_5000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_5000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_5000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "The dark red lines show how things changed when we added an additional **2000** epochs to the training, for a total of **5000** epochs. Now we see that the **loss** (`train_loss`) and the predictions for each company apper to be tapering off, suggesting that adding more epochs may not improve the predictions much, so we're done!\n",
    "\n",
    "Lastly, let's print out the final estimates for the Weights and Biases. In theory, they should be the same (within rounding error) as what we used in the **StatQuest** on **Long Short-Term Memory** and seen in the diagram of the **LSTM** unit at the top of this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4e6b4-7f55-4946-8e76-7a52cf2a39b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "wlr1 tensor(2.7043)\n",
      "wlr2 tensor(1.6307)\n",
      "blr1 tensor(1.6234)\n",
      "wpr1 tensor(1.9983)\n",
      "wpr2 tensor(1.6525)\n",
      "bpr1 tensor(0.6204)\n",
      "wp1 tensor(1.4122)\n",
      "wp2 tensor(0.9393)\n",
      "bp1 tensor(-0.3217)\n",
      "wo1 tensor(4.3848)\n",
      "wo2 tensor(-0.1943)\n",
      "bo1 tensor(0.5935)\n"
     ]
    }
   ],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1dbfc-ba9b-4eb3-9906-71a0edebc43d",
   "metadata": {},
   "source": [
    "## DOUBLE BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff877386-0b56-4c60-bcd1-bfa123b9fe7f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be30faf-c262-448a-b60d-8455ff9af997",
   "metadata": {},
   "source": [
    "<a id=\"nnLSTM\"></a>\n",
    "# Using and optimzing the PyTorch LSTM, nn.LSTM()\n",
    "\n",
    "Now that we know how to create an LSTM unit by hand, train it, and then use it to make good predictions, let's learn how to take advantage of PyTorch's `nn.LSTM()` function. For the most part, using `nn.LSTM()` allows us to simplify the `__init__()` function and the `forward()` function. The other big difference is that this time, we're not going to try and recreate the parameter values we used in the **StatQuest** on **Long Short-Term Memory**, and that means we can set the learning rate for the Adam to **0.1**. This will speed up training a lot. Everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781c6a1-035f-4149-b509-bf808855fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "         \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector\n",
    "        input_trans = input.view(len(input), 1)\n",
    "        \n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5899c78-1e93-45f3-96a5-c17654f507cc",
   "metadata": {},
   "source": [
    "Now let's create the model and print out the initial Weights and Biases and predictinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbb364-34a2-4896-a788-bbd4f8621ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873]])\n",
      "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
      "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([0.6675])\n",
      "Company B: Observed = 1, Predicted = tensor([0.6665])\n"
     ]
    }
   ],
   "source": [
    "model = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "    \n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1cf65-09ca-4e94-b629-e606c6030953",
   "metadata": {},
   "source": [
    "As expected, the predictions are bad, so we will train the model. However, because we've increased the learning rate to **0.1**, we only need to train for **300** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c3792-9b36-4dd0-aeb2-dfbf96dc931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params | Mode \n",
      "--------------------------------------\n",
      "0 | lstm | LSTM | 16     | train\n",
      "--------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 246.04it/s, v_num=41]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 187.35it/s, v_num=41]\n",
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[3.5364],\n",
      "        [1.3869],\n",
      "        [1.5390],\n",
      "        [1.2488]])\n",
      "lstm.weight_hh_l0 tensor([[5.2070],\n",
      "        [2.9577],\n",
      "        [3.2652],\n",
      "        [2.0678]])\n",
      "lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])\n",
      "lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])\n"
     ]
    }
   ],
   "source": [
    "## NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.\n",
    "## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    "## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,\n",
    "## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)\n",
    "## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So\n",
    "trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfd472-5e19-410f-adbe-06ff89ac2efd",
   "metadata": {},
   "source": [
    "Now that training is done, let's print out the new predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fa0f2-a6a1-4806-a3e7-0d670d7f30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([6.8435e-05])\n",
      "Company B: Observed = 1, Predicted = tensor([0.9809])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41c31b-a69f-49bb-8faf-77df15a9e4f3",
   "metadata": {},
   "source": [
    "...and, as we can see, after just **300** epochs, the LSTM is making great predictions. The prediction for **Company A** is close to the observed value **0** and the prediction for **Company B** is close to the observed value **1**.\n",
    "\n",
    "Lastly, let's refresh the **TensorBoard** page to see the latest graphs. **NOTE:** To make it easier to see what we just did, deselect `version_0`, `version_1` and `version_2` and make sure `version_3` is checked on the left-hand side of the page, under where it says `Runs`. See below. This allows us to just look at the log files from the most rescent training, which only went for **300** epochs.\n",
    "\n",
    "<img src=\"./images/selecting_run_version_3.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n",
    "<img src=\"./images/train_loss_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_0_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_1_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f184fa-befe-4c9e-a7f5-e718ab84ea6d",
   "metadata": {},
   "source": [
    "In all three graphs, the loss (`train_loss`) and the predictions for **Company A** (`out_0`) and **Company B** (`out_1`) started to taper off after **500** steps, or just **250** epochs, suggesting that adding more epochs may not improve the predictions much, so we're done!\n",
    "\n",
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
